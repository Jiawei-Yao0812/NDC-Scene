<!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
    <meta charset="utf-8">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PY21MN7T6R"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-PY21MN7T6R');
    </script>

    <meta name="author" content="Jiawei Yao">
    <meta name="description" content="NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized Device Coordinates Space">    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>

    <!-- Primary Meta Tags -->
    <title>NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized Device Coordinates Space</title>
    <meta name="title" content="NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized Device Coordinates Space">
    <meta name="description" content="Monocular 3D Semantic Scene Completion (SSC) has garnered significant attention in recent years due to its potential to predict complex semantics and geometry shapes from a single image, requiring no 3D inputs. In this paper, we identify several critical issues in current state-of-the-art methods, including the Feature Ambiguity of projected 2D features in the ray to the 3D space, the Pose Ambiguity of the 3D convolution, and the Computation Imbalance in the 3D convolution across different depth levels. To address these problems, we devise a novel Normalized Device Coordinates scene completion network (NDC-Scene) that directly extends the 2D feature map to a Normalized Device Coordinates (NDC) space, rather than to the world space directly, through progressive restoration of the dimension of depth with deconvolution operations. Experiment results demonstrate that transferring the majority of computation from the target 3D space to the proposed normalized device coordinates space benefits monocular SSC tasks. Additionally, we design a Depth-Adaptive Dual Decoder to simultaneously upsample and fuse the 2D and 3D feature maps, further improving overall performance. Our extensive experiments confirm that the proposed method consistently outperforms state-of-the-art methods on both outdoor SemanticKITTI and indoor NYUv2 datasets.">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://jiawei-yao0812.github.io/NDC-Scene/">
    <meta property="og:title" content="NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized Device Coordinates Space">
    <meta property="og:description" content="Monocular 3D Semantic Scene Completion (SSC) has garnered significant attention in recent years due to its potential to predict complex semantics and geometry shapes from a single image, requiring no 3D inputs. In this paper, we identify several critical issues in current state-of-the-art methods, including the Feature Ambiguity of projected 2D features in the ray to the 3D space, the Pose Ambiguity of the 3D convolution, and the Computation Imbalance in the 3D convolution across different depth levels. To address these problems, we devise a novel Normalized Device Coordinates scene completion network (NDC-Scene) that directly extends the 2D feature map to a Normalized Device Coordinates (NDC) space, rather than to the world space directly, through progressive restoration of the dimension of depth with deconvolution operations. Experiment results demonstrate that transferring the majority of computation from the target 3D space to the proposed normalized device coordinates space benefits monocular SSC tasks. Additionally, we design a Depth-Adaptive Dual Decoder to simultaneously upsample and fuse the 2D and 3D feature maps, further improving overall performance. Our extensive experiments confirm that the proposed method consistently outperforms state-of-the-art methods on both outdoor SemanticKITTI and indoor NYUv2 datasets.">
    <!-- <meta property="og:image" content="./imgs/SemKITTI.gif"> -->

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://jiawei-yao0812.github.io/NDC-Scene/">
    <meta property="twitter:title" content="NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized Device Coordinates Space">
    <meta property="twitter:description" content="Monocular 3D Semantic Scene Completion (SSC) has garnered significant attention in recent years due to its potential to predict complex semantics and geometry shapes from a single image, requiring no 3D inputs. In this paper, we identify several critical issues in current state-of-the-art methods, including the Feature Ambiguity of projected 2D features in the ray to the 3D space, the Pose Ambiguity of the 3D convolution, and the Computation Imbalance in the 3D convolution across different depth levels. To address these problems, we devise a novel Normalized Device Coordinates scene completion network (NDC-Scene) that directly extends the 2D feature map to a Normalized Device Coordinates (NDC) space, rather than to the world space directly, through progressive restoration of the dimension of depth with deconvolution operations. Experiment results demonstrate that transferring the majority of computation from the target 3D space to the proposed normalized device coordinates space benefits monocular SSC tasks. Additionally, we design a Depth-Adaptive Dual Decoder to simultaneously upsample and fuse the 2D and 3D feature maps, further improving overall performance. Our extensive experiments confirm that the proposed method consistently outperforms state-of-the-art methods on both outdoor SemanticKITTI and indoor NYUv2 datasets.">
    <!-- <meta property="twitter:image" content="./imgs/SemKITTI.gif"> -->

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lato&display=swap" rel="stylesheet">
    
    

    <title>NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized Device Coordinates Space</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <link href="./style.css" rel="stylesheet">
</head>

<body>
    <div class="container" style="text-align:center; padding:10px">        
        <div class="row" style="text-align:center;margin-top: 30px;">
            <h1>NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized Device Coordinates Space</h1>            
        </div>      
        <div class="row" style="text-align:center;margin-top: 30px;">
            <h3>ICCV 2023</h3>            
        </div>     
        <div class="row" style="text-align:center;margin-top: 30px;">
            <h5>
                <div class="author">
                    <a href="" target="_blank">Jiawei Yao</a><sup>1</sup>*,&nbsp;
                    <a href="https://scholar.google.com.sg/citations?user=ZfB7vEcAAAAJ&hl=en" target="_blank">Chuming Li</a><sup>2,4</sup>*,&nbsp;
                    <a href="https://keqiangsun.github.io" target="_blank">Keqiang Sun</a><sup>3</sup>*,&nbsp;
                    <a href="https://yjcaimeow.github.io/" target="_blank">Yingjie Cai</a><sup>3</sup>,&nbsp;
                    <br>
                    <a href="https://cpsxhao.github.io/" target="_blank">Hao Li</a><sup>3</sup>,&nbsp;
                    <a href="https://wlouyang.github.io/" target="_blank">Wanli Ouyang</a><sup>4&dagger;</sup>,&nbsp;
                    <a href="http://www.ee.cuhk.edu.hk/~hsli/" target="_blank">Hongsheng Li</a><sup>3,4,5&dagger;</sup>
                  </div>
                  <br>
                  <!-- <br> -->
                  <div class="institution">
                    <sup>1</sup> University of Washington,
                    <sup>2</sup> The University of Sydney <br>
                    <sup>3</sup> CUHK-SenseTime Joint Laboratory,
                    <sup>4</sup> Shanghai Al Laboratory, <br>
                    <sup>5</sup> CPII under InnoHK <br>
                  </div>
            </h5>
            
            <div style="height: 10px;"></div>
        </div>

        <div class="sep"></div>
        
        <h5>
            <div class="link">
                <a href="" target="_blank">[Paper]</a>&nbsp;
                <a href="" target="_blank">[Supplementary Material]</a>&nbsp;
                <a href="https://github.com/Jiawei-Yao0812/NDCScene" target="_blank">[Code]</a>
            </div>
        </h5>
        

        <div style="height: 50px;"></div>

        <div class="row" style="text-align:center; ">                  
            <h2>Abstract</h2>       
            <div class="col-2 offset-5 underline">
                <hr>
            </div>            
            <div class="col-lg-6 offset-lg-3 col-sm-12" style="text-align: justify;"> 
                Monocular 3D Semantic Scene Completion (SSC) has garnered significant attention in recent years due to its potential to predict complex semantics and geometry shapes from a single image, requiring no 3D inputs. In this paper, we identify several critical issues in current state-of-the-art methods, including the Feature Ambiguity of projected 2D features in the ray to the 3D space, the Pose Ambiguity of the 3D convolution, and the Computation Imbalance in the 3D convolution across different depth levels. To address these problems, we devise a novel Normalized Device Coordinates scene completion network (NDC-Scene) that directly extends the 2D feature map to a Normalized Device Coordinates (NDC) space, rather than to the world space directly, through progressive restoration of the dimension of depth with deconvolution operations. Experiment results demonstrate that transferring the majority of computation from the target 3D space to the proposed normalized device coordinates space benefits monocular SSC tasks. Additionally, we design a Depth-Adaptive Dual Decoder to simultaneously upsample and fuse the 2D and 3D feature maps, further improving overall performance. Our extensive experiments confirm that the proposed method consistently outperforms state-of-the-art methods on both outdoor SemanticKITTI and indoor NYUv2 datasets.
            </div>           
        </div>
    
        

        <div style="height: 50px;"></div>
    
        <div class="row" style="text-align:center; ">            
            <h2>Demo video</h2>       
            <div class="col-2 offset-5 underline">
                <hr>
            </div>    
            <div class="col-lg-10 offset-lg-1 col-md-12">
                <!-- <video style="width: 100%; height: auto;" controls src="imgs/monoscene_demo.mp4"></video> -->
                <div class="video-container">
                    <iframe src="https://www.youtube.com/embed/qh7La1tRJmE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>
            </div>            
        </div>

    
        <div style="height: 50px;"></div>
    
        <div class="row" style="text-align:center; ">      
            <h2>Method</h2>   
            <div class="col-2 offset-5 underline">
                <hr>
            </div>    
            <div class="col-lg-10 offset-lg-1 col-sm-12">
                <img src="imgs/framework.png" width="100%" alt="NDC-Scene framework">                
            </div>  
            <div class="col-lg-10 offset-lg-1 col-sm-12" style="padding-top: 15px;padding-bottom: 15px;">
                <h5>NDC-Scene framework</h5>
                <br>
                <div style="text-align: justify;"> We first exploit an 2D image encoder to produce multi-scale 2D feature maps, followed by our Depth-Adaptive Dual Decoder to restore the 3D feature map, which is further re-projected to the target space to predict the SSC result via a light-weight 3D UNet and a class head.
                </div>
            </div>           
        </div>
        <div class="row">        
            <div class="col-lg-5 offset-lg-1 col-md-6 col-sm-12">
                <img src="./imgs/Depth-adaptive dual decoder.png" class="card-img-top" style="width: 85%;" alt="Depth-adaptive dual decoder">
                <div class="card-body">
                    <p class="card-text">
                        <h5>Depth-adaptive dual decoder</h5> 
                        <br>
                        <div style="text-align: justify;">
                            We infer the initial 3D feature map via a simple reshaping operation, and take the final feature map from the 2D image encoder as the the initial 2D feature map.
                        </div>
                    </p>
                </div>
            </div>
            <div class="col-lg-5 col-md-6  col-sm-12">
                <img src="./imgs/Depth-adaptive attention.png" class="card-img-top" style="width: 80%;" alt="Depth-adaptive attention">
                <div class="card-body">
                    <p class="card-text">
                        <h5>Depth-adaptive attention</h5> 
                        <br>
                        <div style="text-align: justify;">
                            We infer the attention matrix via the inner-production between the 3D query feature and the 2D key feature on each group. We omit the value projection for computation reduction
                        </div>
                    </p>
                </div>
            </div>
            
        </div>

        <div class="sep"></div>

        <div class="row">  
            <h2>Qualitative results</h2>   
            <div class="col-2 offset-5 underline">
                <hr>
            </div> 
            <div class="col-lg-12 col-sm-12" style="margin-top:10px;">
                <img src="./imgs/NYUv2.jpg" class="card-img-top" alt="NYUv2 qualitative results" style="width: 90%;"/>
                <h6 style="margin-bottom: 15px;">NYUv2 (test set)</h6>
                <img src="./imgs/SemanticKITTI.jpg" class="card-img-top" alt="Semantic KITTI qualitative results" style="width: 90%;">
                <h6>Semantic KITTI (validation set)</h6>
            </div>            
        </div>
        
        <div style="height: 50px;"></div>


        
        <div class="row">  
            <h2>Bibtex</h2>   
            <div class="col-2 offset-5 underline">
                <hr>
            </div> 
            <div class="col-lg-8 offset-lg-2 col-sm-12">
                <div>
                    If you find this project useful for your research, please cite
                    <div class="card-block" style="background-color: #f5f5f5; padding:8px;">
                        <!-- <div class="card-text" style="text-align: left;background-color: #f5f5f5;">                                                 -->
<pre class="card-text" style="text-align: left;">
    @inproceedings{todo,
        title={todo}, 
        author={todo},
        booktitle={todo},
        year={todo}
    }
</pre>
                        <!-- </div> -->
                    </div>
                </div>
            </div>
        </div>


        
        
        <div style="height: 50px;"></div>


        <div class="row" style="text-align:center; ">      
            <h2>Acknowledgements</h2>    
            <div class="col-2 offset-5 underline">
                <hr>
            </div>     
            <div class="col-12"></div>
            <div class="col-lg-8 offset-lg-2 col-sm-12" style="text-align:justify;">
                This project is funded in part by National Key R&D Program of China Project 2022ZD0161100, by the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)’s InnoHK, by General Research Fund of Hong Kong RGC Project 14204021. Hongsheng Li is a PI of CPII under the InnoHK.
            </div>           
        </div>
        
        <!-- <div style="height: 50px;"></div> -->

        <div class="row" style="text-align:center; ">      
            <!-- <h2>Copyright Notice</h2>     -->
            <div class="col-2 offset-5 underline">
                <hr>
            </div>     
            <div class="col-12"></div>
            <div class="col-lg-8 offset-lg-2 col-sm-12" style="text-align: justify;">
                This project is built based on MonoScene. We thank the contributors of the prior project for building such excellent codebase and repo. Please refer to this <a href="https://github.com/astra-vision/MonoScene" target="_blank">repo</a> for more documentations and details.
            </div>           
        </div>
    </div>
   
    
    <div style="height: 100px;"></div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous"></script>
    </body>
</html>
